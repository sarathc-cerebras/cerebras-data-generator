
models_to_run:
  #- "qwen-3-235b-a22b-instruct-2507"
  - "qwen-3-32b"

# Concurrency and Rate Limiting
concurrency:
  max_concurrent_requests: 50
  min_concurrent_requests: 20
  recover_threshold: 10 # Number of fast requests before increasing concurrency
  initial_retry_delay: 1.0
  max_retry_delay: 60.0
  max_retries: 5

# Rate limits based on model capacity
# rate_limit:
#   requests_per_minute: 400
#   tokens_per_minute: 2000000

# Load Monitoring
load_monitoring:
  ttft_threshold: 5.0
  request_timeout: 120.0


generation:
  stream: false
  max_completion_tokens: 8196 #20000
  temperature: 0.7
  top_p: 0.8
  seed: 1


# Input and Output files
dataset:
  repo_type: hf
  output: "data/output/baai_infinity_instruct_core_generated.jsonl"
  batch_size: 100 # Number of prompts to process before saving results
  n_samples: 1
  hf:
    repo: "BAAI/Infinity-Instruct" 
    subset: "7M_core"
    split: "train"
    num_proc: 90
  local:
    format: "parquet"
    data_files: "../data/input/gsm8k_test.parquet"
    split: "test"
  # take: 3
  # rename_columns:
  #   - ["conversations", "prompt"]